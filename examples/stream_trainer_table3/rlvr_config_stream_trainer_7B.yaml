hydra:
  run:
    dir: .
  output_subdir: null
exp_name: stream_trainer_qwen2_5_7B_8k
seed: 42
logging_dir: ./output/logs
output_dir: ./output
report_length_and_rewards: true
profiler_output_dir: ./output

system_envs:
  SKIP_OLD_LOG_PROBS: 'True'
  NVTE_TORCH_COMPILE: '0'
  USE_MODELSCOPE: '0'
  LAYER_BY_LAYER: '1'
  LAYER_BY_LAYER_CACHE_RATIO: '0.5'
  ADAPTIVE_TIMEOUT: '1'
checkpoint_config:
  type: file_system
  output_dir: /root/code/${exp_name}

track_with: wandb
tracker_kwargs:
  api_key: your-wandb-api-key
  project: your-project-name
  name: qwen2.5-7B-8k-e2e-performance
  notes: end-to-end RollPacker
  tags:
    - RollPacker
    - CodeMathJudge
    - PromptSqueezer

num_gpus_per_node: 8
multi_infer_tp: false
autoscaling: true
infer_scaling_down_progress_ratio: 0.40
scaling_down_train_batch_size: 64
max_steps: 105
debug_max_steps: 5
save_steps: 100
logging_steps: 1
eval_steps: 1
resume_from_checkpoint: false
global_template: longCOT_qwen2.5
rollout_batch_size: 128
prompt_length: 2048
response_length: 8192
num_return_sequences_in_group: 8
ppo_epochs: 1
adv_estimator: reinforce
value_clip: 0.5
reward_clip: 10
advantage_clip: 2
dual_clip_loss: true
reward_norm: null
reward_shift: false
reward_scale: false
max_len_mask: true
difficulty_mask: true
difficulty_low_threshold: 0.1
difficulty_high_threshold: 0.95
error_max_len_clip: false
difficulty_loss_weight: false
length_loss_weight: false
add_token_level_kl: false
whiten_advantages: true
max_running_requests: 2048
dynamic_sampling: false
max_num_return_sequences: 8
max_prompts: 2048
max_prompts_ratio: 1.0
is_num_return_sequences_expand: true
pretrain: Qwen/Qwen2.5-7B
reward_pretrain: Qwen/Qwen2.5-7B

validation:
  data_args:
    template: ${global_template}
    file_name:
      - /data/oss_bucket_1/rl_examples/data/livecodebench_v5_deal.jsonl
      - data/math_benchmarks.jsonl
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.6
    top_k: 50
    num_beams: 1
    temperature: 0.6
    num_return_sequences: 1

actor_train:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: null
  training_args:
    learning_rate: 5.0e-07
    weight_decay: 0
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 128
    warmup_steps: 20
    num_train_epochs: 50
  data_args:
    template: ${global_template}
    file_name:
      - data/math_deepmath_deal_e2e.jsonl
      - data/llm_judge_math_rlvr_e2e.jsonl
      - data/code_KodCode_data_with_time_e2e.jsonl
    domain_interleave_probs:
      math_rule: 0.34375
      llm_judge: 0.34375
      code_sandbox: 0.3125
    dataset_dir: data
    messages: messages
    interleave_probs: '1.0'
    preprocessing_num_workers: 16
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      context_parallel_size: 1
      overlap_grad_reduce: false
      use_distributed_optimizer: true
      sequence_parallel: true
      moe_token_dispatcher_type: alltoall
      variable_seq_lengths: true
  device_mapping: list(range(0, 8))
  infer_batch_size: 1
  use_remove_padding: true
actor_infer:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.99
    num_return_sequences: ${num_return_sequences_in_group}
  data_args:
    template: ${global_template}
  strategy_args:
    strategy_name: vllm
    strategy_config:
      disable_log_stats: false
      gpu_memory_utilization: 0.85
      block_size: 16
      max_model_len: 32768
      enforce_eager: false
      tensor_parallel_size: 2
      max_num_seqs: 2048
  device_mapping: list(range(0, 8))
  infer_batch_size: 1
reference:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: null
  data_args:
    template: ${global_template}
  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
  device_mapping: list(range(0, 8))
  infer_batch_size: 8
rewards:
  math_rule:
    worker_cls: roll.pipeline.rlvr.rewards.math_rule_reward_worker.MathRuleRewardWorker
    model_args:
      model_name_or_path: ${reward_pretrain}
    data_args:
      template: ${global_template}
    tag_included:
      - deepmath_103k
      - aime
    world_size: 32
    infer_batch_size: 1
  code_sandbox:
    use_local: true
    worker_cls: roll.pipeline.rlvr.rewards.code_sandbox_reward_worker.CodeSandboxRewardWorker
    tag_included: [KodCode]
    model_args:
      model_name_or_path: ${reward_pretrain}
    data_args:
      template: ${global_template}
    world_size: 32
    infer_batch_size: 1
  llm_judge:
    worker_cls: roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker
    judge_prompt: Qwen2.5-7B-Instruct-RLVR-prompt
    judge_model_type: inference
    tag_included:
      - RLVR
    model_args:
      model_name_or_path: virtuoussy/Qwen2.5-7B-Instruct-RLVR
      attn_implementation: fa2
      disable_gradient_checkpointing: true
      dtype: bf16
      model_type: trl
    generating_args:
      max_new_tokens: 1
      top_p: 0.8
      top_k: 50
      num_beams: 1
      temperature: 0.8
      num_return_sequences: 1
    data_args:
      template: ${global_template}
    strategy_args:
      strategy_name: hf_infer
      strategy_config: null
    device_mapping: list(range(0, 8))
    infer_batch_size: 1